import OpenAI from 'openai';
import { WorkflowNode, WorkflowExecutionContext, ConnectionMap, NodeExecutionResult } from '../core/types';
import { 
  formatClinicDataForGPT, 
  getProcedureInfoForGPT, 
  getInsuranceInfoForGPT, 
  getLocationInfoForGPT 
} from '../utils/clinicDataFormatter';

/**
 * Executes a GPT_RESPONSE node that generates complete, contextualized responses
 * Uses clinicData.ts directly to provide rich context to GPT
 */
export async function executeGPTResponseNode(
  node: WorkflowNode,
  context: WorkflowExecutionContext,
  connections: ConnectionMap
): Promise<NodeExecutionResult> {
  const apiKey = process.env.OPENAI_API_KEY;
  
  console.log(`ü§ñ [GPT Response] Starting GPT response generation: ${node.id}`);
  
  if (!apiKey) {
    console.error('ü§ñ [GPT Response] OPENAI_API_KEY not configured');
    context.workflowLogs.push('‚ùå OPENAI_API_KEY n√£o configurada');
    return { 
      nextNodeId: undefined, 
      response: 'Desculpe, servi√ßo temporariamente indispon√≠vel.', 
      shouldStop: true 
    };
  }
  
  const userMessage = (context.message || '').trim();
  const clinicCode = context.userData.selectedClinic || context.userData.clinicCode || 'vieiralves';
  
  // Check for generic/empty messages
  const genericMessages = ['oi', 'ol√°', 'ola', 'hey', 'hi', 'hello', 'ok', 'okay', 'beleza'];
  const isGenericMessage = genericMessages.some(g => 
    userMessage.toLowerCase() === g || 
    userMessage.toLowerCase().length <= 10 && userMessage.toLowerCase().includes(g)
  );
  
  if (!userMessage || isGenericMessage) {
    console.log(`ü§ñ [GPT Response] Skipping GPT for generic/empty message: "${userMessage}"`);
    return { 
      nextNodeId: undefined, 
      response: '', 
      shouldStop: true 
    };
  }
  
  try {
    const client = new OpenAI({ apiKey });
    
    // Get clinic data context
    const clinicContext = formatClinicDataForGPT(clinicCode);
    
    // Build conversation history
    const historyContext = context.conversationHistory
      .slice(-4)
      .map(h => `${h.role === 'user' ? 'Usu√°rio' : 'Bot'}: ${h.content}`)
      .join('\n');
    
    // Detect what the user is asking about
    const normalizedMessage = userMessage.toLowerCase();
    
    // Check if asking about specific procedure
    const procedureKeywords = ['acupuntura', 'fisioterapia', 'rpg', 'pilates', 'quiropraxia', 
                                'libera√ß√£o', 'miofascial', 'ventosa', 'ortop√©dica', 'ortopedica',
                                'neurol√≥gica', 'neurologica', 'p√©lvica', 'pelvica', 'respirat√≥ria', 'respiratoria'];
    const mentionedProcedure = procedureKeywords.find(kw => normalizedMessage.includes(kw));
    
    // Check if asking about insurance
    const insuranceKeywords = ['conv√™nio', 'convenio', 'plano', 'bradesco', 'sulamerica', 
                               'unimed', 'amil', 'aceita', 'atende'];
    const mentionedInsurance = insuranceKeywords.find(kw => normalizedMessage.includes(kw));
    
    // Check if asking about location
    const locationKeywords = ['localiza√ß√£o', 'localizacao', 'endere√ßo', 'endereco', 'onde fica', 
                              'como chegar', 'hor√°rio', 'horario', 'funcionamento'];
    const mentionedLocation = locationKeywords.find(kw => normalizedMessage.includes(kw));
    
    // Check if asking about prices
    const priceKeywords = ['valor', 'pre√ßo', 'preco', 'quanto custa', 'quanto √©', 'quanto custa'];
    const mentionedPrice = priceKeywords.find(kw => normalizedMessage.includes(kw));
    
    // Build specialized context based on what user is asking
    let specializedContext = '';
    
    if (mentionedProcedure && (mentionedPrice || normalizedMessage.includes('qual') || normalizedMessage.includes('quanto'))) {
      // User asking about specific procedure price/info
      const procedureInfo = getProcedureInfoForGPT(mentionedProcedure, clinicCode);
      if (procedureInfo) {
        specializedContext = `\n\nüìã INFORMA√á√ÉO ESPEC√çFICA DO PROCEDIMENTO:\n${procedureInfo}`;
      }
    } else if (mentionedInsurance) {
      // User asking about insurance
      const insuranceInfo = getInsuranceInfoForGPT(mentionedInsurance);
      specializedContext = `\n\nüè• INFORMA√á√ÉO SOBRE CONV√äNIO:\n${insuranceInfo}`;
    } else if (mentionedLocation) {
      // User asking about location
      const locationInfo = getLocationInfoForGPT(clinicCode);
      specializedContext = `\n\nüìç INFORMA√á√ÉO DE LOCALIZA√á√ÉO:\n${locationInfo}`;
    }
    
    // Build system prompt
    const systemPrompt = `Voc√™ √© um assistente virtual especializado da ${clinicData.name}. 
Seu papel √© fornecer informa√ß√µes completas, precisas e √∫teis sobre a cl√≠nica.

${clinicContext}

${specializedContext}

**INSTRU√á√ïES IMPORTANTES:**
1. Use APENAS as informa√ß√µes fornecidas acima - n√£o invente dados
2. Seja completo mas objetivo - forne√ßa todas as informa√ß√µes relevantes
3. Use linguagem clara, amig√°vel e profissional
4. Se o usu√°rio perguntar sobre valores, sempre mencione:
   - Valor particular
   - Se requer avalia√ß√£o pr√©via e seu custo
   - Pacotes dispon√≠veis (se houver)
   - Que valores com conv√™nio podem variar
5. Se perguntar sobre procedimentos, explique:
   - O que √© o procedimento
   - Para que serve
   - Dura√ß√£o
   - Se requer avalia√ß√£o
6. Sempre ofere√ßa agendamento ao final da resposta quando apropriado
7. Se n√£o souber algo, seja honesto e sugira entrar em contato

**FORMATO DA RESPOSTA:**
- Seja natural e conversacional
- Use emojis quando apropriado (üí∞, üè•, üìç, ‚è∞, etc)
- Organize informa√ß√µes de forma clara
- Antecipe perguntas comuns do usu√°rio

Responda de forma completa e √∫til, usando TODAS as informa√ß√µes relevantes do contexto acima.`;
    
    // Build user prompt
    const userPrompt = `Hist√≥rico da conversa:\n${historyContext || '(sem hist√≥rico anterior)'}\n\n` +
                      `Mensagem atual do usu√°rio: "${userMessage}"\n\n` +
                      `Responda de forma completa, usando todas as informa√ß√µes relevantes do contexto da cl√≠nica.`;
    
    context.workflowLogs.push(`ü§ñ [GPT Response] üì® Mensagem: "${userMessage}"`);
    context.workflowLogs.push(`ü§ñ [GPT Response] üè• Cl√≠nica: ${clinicCode}`);
    context.workflowLogs.push(`ü§ñ [GPT Response] ‚è≥ Gerando resposta completa...`);
    
    // Use more powerful model for complex responses (gpt-4o or gpt-4-turbo)
    const responseModel = process.env.OPENAI_RESPONSE_MODEL || 'gpt-4o';
    
    console.log(`ü§ñ [GPT Response] Using model: ${responseModel} for contextual response`);
    context.workflowLogs.push(`ü§ñ [GPT Response] üìä Modelo usado: ${responseModel}`);
    
    const completion = await client.chat.completions.create({
      model: responseModel,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature: 0.7, // Slightly higher for more natural responses
      max_tokens: 600 // More tokens for complete rich responses
    }, { timeout: 30000 });
    
    const response = completion.choices?.[0]?.message?.content || '';
    
    context.workflowLogs.push(`ü§ñ [GPT Response] ‚úÖ Resposta gerada (${response.length} caracteres)`);
    
    // Get next node
    const nodeConnections = connections.get(node.id) || [];
    const nextNodeId = nodeConnections[0]?.targetId;
    
    // Add response to history
    if (response.trim()) {
      context.conversationHistory.push({
        role: 'bot',
        content: response
      });
    }
    
    return {
      nextNodeId,
      response: response.trim(),
      shouldStop: true, // Stop to show response
      shouldSaveNextNode: true
    };
    
  } catch (error: any) {
    console.error(`ü§ñ [GPT Response] Error:`, error);
    context.workflowLogs.push(`ü§ñ [GPT Response] ‚ùå ERRO: ${error.message}`);
    
    return {
      nextNodeId: undefined,
      response: 'Desculpe, tive um problema ao processar sua mensagem. Pode repetir?',
      shouldStop: true
    };
  }
}

// Import clinicData for reference in system prompt
import { clinicData } from '../../../data/clinicData.js';

